{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4MbdUH_dzde"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Understanding the problem\n",
        "\"\"\"\n",
        "We're going to code Linear Regression from scratch with numpy and pure math via `Python`. We gon draw via `matplotlib`\n",
        "\n",
        "Details:\n",
        "Linear Regression is the way to model the realtionship between dependent(y aka prediction) and independet(x aka input)\n",
        "house_sizes = [1000, 1200, 1400, 1600]  # These are x values\n",
        "house_prices = [150, 180, 210, 240]     # These are y values\n",
        "\n",
        "We can use `house_sizes` in order to predict `house_prices`:\n",
        "  After training(our model) we can come up with a solution like below:\n",
        "  Price= 0.1 * Size + 50 ---> 0.1*100 = 100 + 50 = 150(house_prices[0])\n",
        "  So `y` = 0.1 * x + 50\n",
        "  `m` here is our slope(aka weight) which means how line aligns when we play with `x` via `m`. Weight controls how the line changes it's `direction and steepness`  without changing it is starting point(aka b)\n",
        "  `b` points the value of `y(aka predictions)` when `x`(aka input is 0) also knows as the starting point of our `line`.It either can be origin or down through the `y` axis or up through the `y` axis.\n",
        "\n",
        " Important:\n",
        " If your intercept `b` is in the bad location(aka has bad value/prediction) then it does not matter how good is your slope `m`. Because even though you play with `m` it you will not be able to\n",
        " find the best fit because `data points(aka (x,y))` might be in center meanwhile your `b` is placed on `y` axis point 3. Best fit wont be possible. So  you have to move your `b` to the starting point center(aka origin)\n",
        "\n",
        " Example use case of bad `b`:\n",
        " # Let's say our data looks like this:\n",
        " x = [1, 2, 3, 4, 5]\n",
        " y = [101, 103, 105, 107, 109]  # Data clusters around y = 100 - 110\n",
        "\n",
        " BAD: Wrong value of `b`\n",
        " if b = 3 then y = mx + 3\n",
        " even with perfect slope `m`= 2 , y = 2x + 3\n",
        " Predictions: [5, 7, 9, 11, 13] - WAY off from actual [101, 103, 105, 107, 109]\n",
        "\n",
        " Good: Correct value of `b`\n",
        " # GOOD: Correct b value\n",
        " # If b = 99, then: y = 2*x + 99\n",
        " # Predictions: [101, 103, 105, 107, 109] - Perfect fit!\n",
        "\n",
        " Important:\n",
        " The best `b` depends on the best `m`. Becuase according to the directio/steepness of `m`,  `b` resizes itself to go either up or down\n",
        "\n",
        " For animational info - https://bharathikannann.github.io/blogs/a-visual-intro-to-linear-regression-math/\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Steps to take in order to tackle the problem.\n",
        "\n",
        "| Step | Title                         | Purpose                                      |\n",
        "| ---- | ----------------------------- | -------------------------------------------- |\n",
        "| 1    | Define Dataset (x, y)         | Set your input and target values             |\n",
        "| 2    | Initialize Parameters (w, b)  | Randomly or zero-init the slope & intercept  |\n",
        "| 3    | Predict `y_hat = wx + b`      | Linear prediction formula                    |\n",
        "| 4    | Compute Loss (MSE)            | Measure how bad predictions are              |\n",
        "| 5    | Compute Gradients (dw, db)    | Find how to update w, b to reduce error      |\n",
        "| 6    | Update Parameters             | Apply gradient descent using learning rate   |\n",
        "| 7    | Train for N Epochs            | Loop steps 3 → 6 until convergence           |\n",
        "| 8    | Final Evaluation / Plot (opt) | Check how well the final model fits the data |\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "collapsed": true,
        "id": "5uXDterxd6mF",
        "outputId": "c72e59a6-d57b-416f-ea46-ce6ddfcc7255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nSteps to take in order to tackle the problem.\\n\\n| Step | Title                         | Purpose                                      |\\n| ---- | ----------------------------- | -------------------------------------------- |\\n| 1    | Define Dataset (x, y)         | Set your input and target values             |\\n| 2    | Initialize Parameters (w, b)  | Randomly or zero-init the slope & intercept  |\\n| 3    | Predict `y_hat = wx + b`      | Linear prediction formula                    |\\n| 4    | Compute Loss (MSE)            | Measure how bad predictions are              |\\n| 5    | Compute Gradients (dw, db)    | Find how to update w, b to reduce error      |\\n| 6    | Update Parameters             | Apply gradient descent using learning rate   |\\n| 7    | Train for N Epochs            | Loop steps 3 → 6 until convergence           |\\n| 8    | Final Evaluation / Plot (opt) | Check how well the final model fits the data |\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define our dataset - Set your input and target variables\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([3, 5, 7, 9, 11])\n",
        "\n",
        "\"\"\"\n",
        "According to our dataset we can say that our perfect linear data is `y = 2x + 1` according to our formula `y = mx + b`. So our model should converge to m = 2, b =1. `x` is input, `y` is target.\n",
        "at `y = 2x + 1` line will be perfect fit to the most of our data. This way we can nuemrically and visually can learn if our model is learning correctly.\n",
        "\"\"\"\n",
        "\n",
        "# Step 2: Initalize the parameters (w, b) - Randomly or zero-init the slope & intercept\n",
        "m, b = 0, 0\n",
        "\n",
        "# Step 3: Predict `y_hat = mx + b` - Linear prediction formula\n",
        "def predict(m, x, b):\n",
        "  y_hat = np.array([])\n",
        "  for i in range(len(x)):\n",
        "    y_hat = np.append(y_hat, m*x[i] + b)\n",
        "\n",
        "  return y_hat\n",
        "\n",
        "print(predict(5, x, 3))\n",
        "\n",
        "# Step 4: Compute Loss (MSE) - Measure how bad predictions are. Lookup the formula for MSE before coming here\n",
        "def compute_cost(y_hat, y):\n",
        "  \"\"\"\n",
        "  Here we are comuting loss by subtracting original value that we wanna predict which is y and the predicted value which is y_hat. And then we square it. Why? Well, let's have a look below.\n",
        "  1.When we square our error we get it bigger. Actually we are massaging/preparing our error to be destroyed by Gradient Descent(we gon hav a lookat it soon). Gradient Descent always punishes big erros first and more\n",
        "  2.Squaring helps us to eliminate negative values. Imagine your y_hat - y is -5 and at next step y_hat - y is 5. And? when you will add them together(according to formula) they will eat eachother(-5 + 5 = 0). So you lose prediction\n",
        "  3.Big errors gets punished more\n",
        "\n",
        "  Note: We're going to use this function in order to measure how well our model performs at the end. Each time Gradient Descent updates `w` and `b` we're going to call this function in order to see the error.\n",
        "  The less error is the better model performs.\n",
        "  \"\"\"\n",
        "  # loss = np.square(y_hat - y)/len(y)\n",
        "  loss = np.square(np.subtract(y_hat, y)).mean() # Let's say 139.0 is our mean(aka average) error. But in order ot find true error we gotta sqrt(loss) so we will get 11.8. This means on average, your predictions are about 11.8 units away from the true values\n",
        "  return loss\n",
        "\n",
        "print(compute_cost(predict(5 , x, 3), y))\n",
        "\n",
        "# Compute gradients (dw, db) - Find how to update w and b to reduce error(between y_hat and y)\n",
        "# dw - derivative of w or w.t.r(with respect to) w\n",
        "# db - derivative of b or w.t.r(with respect to) b\n",
        "\n",
        "def compute_gradients(x, y, y_hat):\n",
        "  \"\"\"\n",
        "  We have to use Gradient Descent Mathematical Derivative formula in order to take derivative of `m` and `b` with respect to `loss`(the lost that gradient descent formula has by default inside there). It is not the same loss we get form `compute_cost function`\n",
        "  Derivative means `slope`. Slope is the shape of the line we get by playing with `m`.Check the animation link above to play with `m` and `b`.Despite `b` also has affect when it comes to line , we use `m` to define slope. Why? look below\n",
        "\n",
        "  Why `m` defines `slope` despite `b` is also moving line up and down through `y` axis?\n",
        "  1. When we talk about `slope` we care about steepness(aka where line is tilting). Steepness represents the rate of change. Derivatice is all about `change`.\n",
        "  The steeper the `slope` is the faster is the rate of change\n",
        "  The less stepper/flatter the `slope` is the slower is the rate of change\n",
        "  This relationship helps to understand how output changes when we change the input. Because input is x and output is y so the linewe gon draw via (x,y) has steepness\n",
        "\n",
        "  We are going to take derivative of `m` and `b` with respect to `loss`. We're going to use partial derivative\n",
        "\n",
        "  \"\"\"\n",
        "  n = len(x)\n",
        "  dw = (2/n) * np.dot(np.subtract(y_hat, y), x)\n",
        "  db = (2/n) * np.sum(np.subtract(y_hat, y))\n",
        "\n",
        "  \"\"\"\n",
        "  What we have just done is a magic where two variables `m` and `b` taking their parts to draw the perfect line that fits the data. How? Look below\n",
        "  When we do `dw` with `gradient descent` we actually tell to our model to compute the `slope` while ignoring `b(aka intercept)`. Because in order to draw the best intercept `b` we have find the best slope.\n",
        "  So our model be like okay we took partial derivative w.t.r to `m` which means ignore the `b(aka intercept and either go down or up on the y axis)` because we only care about `steepness` currently\n",
        "\n",
        "  Once we done with `dw` we get to `db`\n",
        "\n",
        "  Our models says okay i found `w` via `dw` so let's draw intercept `b`(aka move up or down through y axis to help  the slope to fit the data) therefore move our line up or down on the `y` axis to find the best fit.\n",
        "  And for each `w` and `b` we keep calling the `compute_cost` in order to understand how many times `dw` and `db` changed `w` and `b` changed.\n",
        "  \"\"\"\n",
        "  return dw, db\n",
        "\n",
        "print(compute_gradients(x, y, predict(5, x, 3)))\n",
        "\n",
        "\"\"\"\n",
        "Important NOTE about `compute_gradient` and what it's result means:\n",
        "This is the result we got from `compute_gradient` function - (np.float64(78.0), np.float64(22.0))\n",
        "\n",
        "78.0 for `dw` - `w`\n",
        "22.0 for `db` - `b`\n",
        "\n",
        "It says - If you move `w` slightly the cost will go up a lot by `78 * by that change`\n",
        "It also says - If you move `b` slightly the cost will go up a lot by `22 * by that change`\n",
        "\n",
        "This huge cost. It means you are not walking you are jumping down the road. You may miss your target. You have to take small steps instead of jumping in order to avoid important miss\n",
        "`miss` here referring to finding the best `w` and `b`. Huge cost(such as 78 and 22) means you are not checking points but you are counting on luck by checking the random points.\n",
        "You may jump over of your  `best fit` line coordinates.\n",
        "And that's why we introduce using something called `learning_rate`\n",
        "\n",
        "`learning_rate` just multiplying your cost gradient with decimal point number and gets it smaller sometimes under `1`.Which tells to your model to take smaller steps down the road so you wont miss any improtant points\n",
        "\n",
        "Typical `learning rate` values to try:\n",
        "\n",
        "learning_rate = 0.01    # Most common starting point\n",
        "learning_rate = 0.001   # Conservative (slower but safer)\n",
        "learning_rate = 0.1     # Aggressive (faster but riskier)\n",
        "learning_rate = 0.0001  # Very conservative\n",
        "\"\"\"\n",
        "\n",
        "# Update parameters - Apply gradient descent using learning rate\n",
        "\n",
        "def update_parameters(m, b, dw, db, learning_rate):\n",
        "  m = m - learning_rate * dw\n",
        "  b = b - learning_rate * db\n",
        "\n",
        "  return m, b\n",
        "\n",
        "\n",
        "print(update_parameters(m, b, *compute_gradients(x, y, predict(5, x, 3)),0.01))\n",
        "\n",
        "\n",
        "# Train for N epochs\n",
        "def train(x, y, epochs, learning_rate):\n",
        "  \"\"\"\n",
        "  We need to now join all those functions we wrote in a loop in order ot keep training our model until it converges.\n",
        "  Steps:\n",
        "  Predict ---> compute loss ---> compute gradients ---> update parameters\n",
        "\n",
        "  Details:\n",
        "  Let's train Linear Regression model with gradient descent\n",
        "  - x: input features (array)\n",
        "  - y: target values (array)\n",
        "\n",
        "  - epochs: number of training iterations\n",
        "  - learning_rate: step size for gradient descent in order to go down the road carefully\n",
        "\n",
        "  Returns:\n",
        "  trained parameters for (m and b)\n",
        "  cost history for plotting later\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize parameters\n",
        "  m, b = 0, 0\n",
        "  cost_history = [] # for storing costs to plot how it has changed while training\n",
        "  print(\"Start training.....\")\n",
        "  print(\"Target: y= 2x + 1\")\n",
        "  print(f\"Initial parameters: m = {m}, b = {b}\")\n",
        "  print(\"-\" * 50)\n",
        "\n",
        "  # Let's start looping\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    # Predict here\n",
        "    y_hat = predict(m, x, b)\n",
        "\n",
        "    # Compute and add cost here\n",
        "    cost = compute_cost(y_hat, y)\n",
        "    cost_history.append(cost)\n",
        "\n",
        "    # Compute gradients\n",
        "    dw, db = compute_gradients(x, y, y_hat)\n",
        "\n",
        "    # Update parameters\n",
        "    m, b = update_parameters(m, b, dw, db, learning_rate)\n",
        "\n",
        "    # Print progress for every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "      print(f\"Epoch {epoch}, Cost = {cost:8.4f}, m = {m:6.3f}, b = {b:6.3f}\")\n",
        "\n",
        "\n",
        "  print(\"-\"*50)\n",
        "  print(\"Training completed!\")\n",
        "  print(f\"Final parameters: m={m:.3f}, b={b:.3f}\")\n",
        "  print(f\"Final cost: {cost_history[-1]:.6f}\")\n",
        "  print(\"Target was m =2, b = 1\")\n",
        "\n",
        "  return m, b, cost_history\n",
        "\n",
        "\n",
        "# Train the model\n",
        "final_m, final_b, cost_history = train(x, y, epochs=1000, learning_rate = 0.01)\n",
        "\n",
        "# Test the final model\n",
        "print(\"\\nTesting final model:\")\n",
        "final_predictions = predict(final_m, x, final_b)\n",
        "print(f\"Predictions: {final_predictions}\")\n",
        "print(f\"Actual:      {y}\")\n",
        "print(f\"Difference:  {final_predictions - y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwTzvdlZz-Wy",
        "outputId": "fd483b42-eda0-4cfb-cd2f-f6bfe09fc8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8. 13. 18. 23. 28.]\n",
            "139.0\n",
            "(np.float64(78.0), np.float64(22.0))\n",
            "(np.float64(-0.78), np.float64(-0.22))\n",
            "Start training.....\n",
            "Target: y= 2x + 1\n",
            "Initial parameters: m = 0, b = 0\n",
            "--------------------------------------------------\n",
            "Epoch 0, Cost =  57.0000, m =  0.500, b =  0.140\n",
            "Epoch 100, Cost =   0.0159, m =  2.082, b =  0.706\n",
            "Epoch 200, Cost =   0.0081, m =  2.058, b =  0.790\n",
            "Epoch 300, Cost =   0.0041, m =  2.041, b =  0.851\n",
            "Epoch 400, Cost =   0.0021, m =  2.030, b =  0.893\n",
            "Epoch 500, Cost =   0.0011, m =  2.021, b =  0.924\n",
            "Epoch 600, Cost =   0.0005, m =  2.015, b =  0.946\n",
            "Epoch 700, Cost =   0.0003, m =  2.011, b =  0.961\n",
            "Epoch 800, Cost =   0.0001, m =  2.008, b =  0.973\n",
            "Epoch 900, Cost =   0.0001, m =  2.005, b =  0.980\n",
            "--------------------------------------------------\n",
            "Training completed!\n",
            "Final parameters: m=2.004, b=0.986\n",
            "Final cost: 0.000036\n",
            "Target was m =2, b = 1\n",
            "\n",
            "Testing final model:\n",
            "Predictions: [ 2.98987045  4.99375103  6.99763161  9.0015122  11.00539278]\n",
            "Actual:      [ 3  5  7  9 11]\n",
            "Difference:  [-0.01012955 -0.00624897 -0.00236839  0.0015122   0.00539278]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plott the line\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(x, y, color='blue', label='Actual Data')             # True data points\n",
        "plt.plot(x, final_predictions, color='red', label='Learned Line')            # Predicted line\n",
        "plt.scatter(x, final_predictions, color='green', marker='x', label='Predictions')  # Predicted dots\n",
        "\n",
        "# Labels and legend\n",
        "plt.title(\"Linear Regression Fit (From Scratch)\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "pq3ipkG6EJAO",
        "outputId": "eaff3289-c1cb-4a9a-c695-4fd51cdc2e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alright cool, now let's try to code Linear Regression with multi-feature support.\n",
        "\n",
        "\"\"\"\n",
        "GOAL and DESCRIPTION:\n",
        "\n",
        "Goal - is to LEVEL UP our existing Linear Regression model from single-feature to multi-feature and predict house prices based on 3(newly added) features\n",
        "\n",
        "Description:\n",
        "\n",
        "Now we are going to be more real and use our Linear Regression model on real world case.We are going to level up our model.\n",
        "First we going to create new `X` and add houses with 3 features and then we are going to have\n",
        "`y` for the actual prices.\n",
        "\n",
        "We are going to modify some functions, therefore  I've decided to code in new column.\n",
        "\n",
        "Step 1: - Explain the change we going to do and apply it.\n",
        "\"\"\"\n",
        "\n",
        "# Step 1 - goes here:\n",
        "\n",
        "# Things to do - Upgrade `X` and `y` with house features and prices.\n",
        "X = np.array([\n",
        "    [1000, 2, 10], # House 1: 1000 sqrft, 2 bedrooms, 10 years\n",
        "    [1200, 3, 5],  # House 2: 1200 sqrft, 3 bedrooms, 5 years\n",
        "    [1400, 2, 20], # House 3: 1400 sqrft, 2 bedrooms, 20 years\n",
        "    [1600, 4, 2]   # House 4: 1600 sqrft, 4 bedrooms, 2 years\n",
        "])\n",
        "\n",
        "# Let's also define our vector(aka array or list) with the actual prices\n",
        "y = np.array([220, 280, 260, 366]) # Exact house prices for each house. We gotta predict closer to these values\n",
        "\n",
        "\"\"\"\n",
        "Each ROW = One house (one sample)\n",
        "Each COLUMN = One feature\n",
        "\n",
        "Column 0: Size (1000, 1200, 1400, 1600)\n",
        "Column 1: Bedrooms (2, 3, 2, 4)\n",
        "Column 2: Age (10, 5, 20, 2)\n",
        "\n",
        "So X[0] = [1000, 2, 10] = \"House 1 has 1000 sqft, 2 bedrooms, 10 years old\"\n",
        "and etc...\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(X)\n",
        "print(\"-\"*50)\n",
        "print(y)\n",
        "\n",
        "# Explaining why this breaks our existing model\n",
        "\n",
        "\"\"\"\n",
        "Well buddy, eyes up here bcz a litte too broad one goes here. HEADS UP\n",
        "\n",
        "Out current `predict` function performs witht he `equation` called y = wx + b.(w and m are the same thing. so let's say w)\n",
        "Heads Up!!! - we have only on `w` there. So? `w` stands for `weight` which means `w` always lets our model to know how much the feature is important. in existing mode we have only one featrure. Look below\n",
        "\n",
        "existing `x` - x = np.array([1, 2, 3, 4, 5]). let's say numbers in our `x` are the bedrooms. So? We are just given 1 feature and it is `bedroom` and we are asked to predict only on that? Bullshit...\n",
        "Since we have only one `feature` which is `bedroom` inherently we only need one `w(aka weight)` in order to explain to our model how much is that one feature `bedroom` is important. You get it? Each time\n",
        "we're going to take on number(bedroom count) from our existing `x` and multiply it with the `w(aka weight)` in order to point out the importance of them - `w*x[i] + b`\n",
        "\n",
        "So where the FUCK is problem here?\n",
        "\n",
        "Problem is in our modified `X`. Becasue we dont have only bedrooms but also sqrft and yearsss. in total `sqrft, bedrooms, year`. You know what does it mean? It means that we gotta have 3 different `w` variables.\n",
        "Why? because each of those features(sqrft, bedrooms, year) are their own importance level. How? Look below\n",
        "\n",
        "Older hosue may decrease the price\n",
        "More bedrooms ends up with high price\n",
        "High sqrft means higher area so higher price.\n",
        "\n",
        "Now you getting it? - Imagine you have only one  `w` and you tryna to point out the importance of all those 3 features with only one variable.WTF is that? So they all gonna have the same importance level?\n",
        "BULLSHITTTTT HAHAHAHA\n",
        "\"\"\"\n",
        "\n",
        "# let's declare our weights now: we are going to creat a list of weights with the length of features which is 3.\n",
        "# w = np.array([0, 0, 0]) # why zeros? Remember that in the beginning our model is dump. It doesnt know anything. It needs to learn. So it is going to learn and update those values in `w` to adjust weights properly\n",
        "\n",
        "# even better apperoach is to actually take the size of features from `X` itself and create array of `w`\n",
        "w = np.zeros(X.shape[1]) # shape[0] going to return the count of rows whichis 4 but shape[1] going to return the count of columns which is 3.\n",
        "b = 0\n",
        "\"\"\"\n",
        "For better intuition:\n",
        "w = [w1, w2, w3] Where:\n",
        "w1 - how much size affects price\n",
        "w2 - how much bedrooms affect price\n",
        "w3 - how much age affects price\n",
        "\n",
        "therefore, Our predictions become:\n",
        "y_hat = w1*size + w2*bedrooms+  w3*age + b or in vector form - y_hat = np.dot(w, X[i] + b) # Doc product\n",
        "\n",
        "\n",
        "Magic of DOT product:\n",
        "\n",
        "for e.g:  w = [0.1, 50, -2] # weights\n",
        "X[0] = [1000, 2, 10] # house 1 with features\n",
        "\n",
        "np.dot(w, X[0]) = 0.1*1000 + 50*2 + (-2)*10 =\n",
        "                = 100 + 100 - 20\n",
        "\n",
        "This says, size adds 100$, bedrooms add 100$, age subtracts 20\n",
        "\n",
        "So, our model now can learn that:\n",
        "\n",
        "Your model can now learn that:\n",
        "\n",
        "Bigger houses cost more (w1 > 0)\n",
        "More bedrooms cost more (w2 > 0)\n",
        "Older houses cost less (w3 < 0)\n",
        "\n",
        "Good job isn't it?\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Step 2: Changing the predict function so it can support multi-features\n",
        "\n",
        "# our old predict function supports single feature\n",
        "\"\"\"\n",
        "def predict(m, X, b):\n",
        "  return m*X + b\n",
        "\"\"\"\n",
        "\n",
        "# Updated function\n",
        "def predict_with_multi_features(w, X, b):\n",
        "  return np.dot(X, w) + b"
      ],
      "metadata": {
        "id": "q3d5o3IQElig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2be0d515-f417-46a1-ca60-fb6dd79dc9f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1000    2   10]\n",
            " [1200    3    5]\n",
            " [1400    2   20]\n",
            " [1600    4    2]]\n",
            "--------------------------------------------------\n",
            "[220 280 260 366]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict_with_multi_features(w, X, b)\n",
        "predictions\n",
        "\n",
        "\"\"\"\n",
        "Perfect now our multi-feature function works. If you wanna see that if it predicts `y` correctly then just grab following things and plug in the function above and run.\n",
        "\n",
        "Things to check:\n",
        "\n",
        "# Note - these are the perfect numbers where I found mathematically. So our model gotta find the same. but not now. Im giving you those so you can unerstand our function. However,\n",
        "our model goig to find those `weights` and `bias` after multiple times of iterations.\n",
        "\n",
        "w = np.array([0.15, 20, -2])  # weights\n",
        "b = 50  # bias\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "qF9OMTfbN6fl",
        "outputId": "d7d42b37-d288-461d-ba4f-9dfd806c95df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPerfect now our multi-feature function works. If you wanna see that if it predicts `y` correctly then just grab following things and plug in the function above and run.\\n\\nThings to check:\\n\\n# Note - these are the perfect numbers where I found mathematically. So our model gotta find the same. but not now. Im giving you those so you can unerstand our function. However,\\nour model goig to find those `weights` and `bias` after multiple times of iterations.\\n\\nw = np.array([0.15, 20, -2])  # weights\\nb = 50  # bias\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute_cost function going to stay same becasue it takes y and y_prediciton as argument and calcs the difference in order to find the error(distance). So it works perfectly well"
      ],
      "metadata": {
        "id": "VWH9f8AimRK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute_gradient modified goes here:\n",
        "\n",
        "# Let's have a quick glimpse to our existing func\n",
        "\n",
        "\"\"\"\n",
        "def compute_gradients(x, y, y_hat):\n",
        "    n = len(x)\n",
        "    dw = (2/n) * np.dot(np.subtract(y_hat, y), x)  # Single feature\n",
        "    db = (2/n) * np.sum(np.subtract(y_hat, y))\n",
        "    return dw, db\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "So, here we are using our existing 1D `w` in order to calcualte the gradients.What is gradient again? Gradient is just a number you get after taking derivative loss((y-y_hat)^2).\n",
        "When u subtract that number from the `weight` it reduces the error.\n",
        "\n",
        "Now, we have modified and upgraded `predict` fucntion that works with multiple features. So ? we no longer can rely on our existin'  `compute_gradient` method.\n",
        "\n",
        "We have to modify it so it's going to be able to calc the error and compute gradient for  2D `X`.\n",
        "\"\"\"\n",
        "\n",
        "# Implementaiton:\n",
        "\n",
        "# `x` becomes `X` and `dw` calcualtion going to change completely\n",
        "\n",
        "def compute_gradients_for_multiple_features(X, y, y_hat):\n",
        "  n = len(x) # number of samples\n",
        "  # let's keep `np.subtract(y_hat, y)` in a variable instead of hardcoding\n",
        "  error = np.subtract(y_hat, y)\n",
        "  dw = (2/n)*np.dot(X.T, error) # I'll explain this aprt because it is dead important. It is the reason why we update this func. Find the explanation with code `Transpose`.\n",
        "  db = (2/n)*np.sum(error) # we controlling the line through the `y` axis.\n",
        "\n",
        "  return dw, db\n",
        "\n",
        "\n",
        "\n",
        "# Transpose:\n",
        "\n",
        "\"\"\"\n",
        "Well, the reason I'm using `X.T` is that i wanna actually swap rows with columns aka houses with features. Look below. I run some codes there.\n",
        "in the bgeinning X is like first row [1000, 2, 10] is `house 1`. one house and all it's features. That's a problem. Becasue when we calculated the `error` in our `compute_gradients_for_multiple_features` function,\n",
        "we have to SUBTRACT it from our features but we ahev to put all same features together first.\n",
        "\n",
        "Such as below:\n",
        "sqrft - [1000, 1200, 1400, 1600] here we can multiply each sqrft with error and add them all toegether and get a value `N`. N means how much error gonna change if you change `w` tiny bit.\n",
        "bedrooms - [2, 3, 2, 4] # same here\n",
        "ages - [10, 5, 20, 2] # same here\n",
        "\n",
        "So, In order to make our X to look like above we use `X.T`.It is numpy and Linear Algebra  practice.Please go and do some practice in numpy and make sure you understand math behind it.\n",
        "\n",
        "\n",
        "SUM IT UP BELOW:\n",
        "\n",
        "What np.dot(X.T, error) Does:\n",
        "Let's say your errors are: error = [10, -5, 8, -3]\n",
        "\n",
        "pythonnp.dot(X.T, error) = np.dot([\n",
        "    [1000, 1200, 1400, 1600],  # Size row\n",
        "    [2, 3, 2, 4],              # Bedroom row\n",
        "    [10, 5, 20, 2]             # Age row\n",
        "], [10, -5, 8, -3])# errors\n",
        "\n",
        "# This gives:\n",
        "# dw[0] = 1000×10 + 1200×(-5) + 1400×8 + 1600×(-3) = gradient for size weight\n",
        "# dw[1] = 2×10 + 3×(-5) + 2×8 + 4×(-3) = gradient for bedroom weight\n",
        "# dw[2] = 10×10 + 5×(-5) + 20×8 + 2×(-3) = gradient for age weight\n",
        "This tells you:\n",
        "\n",
        "How much to change the size weight\n",
        "How much to change the bedroom weight\n",
        "How much to change the age weight\n",
        "\"\"\"\n",
        "\n",
        "# Now let's update update_parameters function\n",
        "\n",
        "def update_parameters_multi_features(w, b, dw, db, learning_rate):\n",
        "    w = w - learning_rate * dw  # w is now array, dw is array\n",
        "    b = b - learning_rate * db  # b is still scalar\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "m51pGP6WoMAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyxXLMyTvenI",
        "outputId": "49871118-7326-4707-b8f9-4d26258d78c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1000,    2,   10],\n",
              "       [1200,    3,    5],\n",
              "       [1400,    2,   20],\n",
              "       [1600,    4,    2]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpTTmLNavfKY",
        "outputId": "57ce28be-8903-4f09-f210-74a7c9c90342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62lYDHqGvhYE",
        "outputId": "aea99216-9888-4c8f-91b3-7a2251bf7beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1000, 1200, 1400, 1600],\n",
              "       [   2,    3,    2,    4],\n",
              "       [  10,    5,   20,    2]])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SCALE X and y ---\n",
        "X_mean = X.mean(axis=0)\n",
        "X_std = X.std(axis=0)\n",
        "X_scaled = (X - X_mean) / X_std\n",
        "\n",
        "y_mean = y.mean()\n",
        "y_std = y.std()\n",
        "y_scaled = (y - y_mean) / y_std\n",
        "\n",
        "# --- INIT weights ---\n",
        "w = np.zeros(X.shape[1])\n",
        "b = 0\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# --- TRAINING LOOP ---\n",
        "print(\"Start training...\")\n",
        "print(f\"Initial weights: {w}, b = {b}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # y_hat = np.dot(X_scaled, w) + b\n",
        "    y_hat = predict_with_multi_features(w, X_scaled, b)\n",
        "    #cost = np.mean((y_scaled - y_hat) ** 2)\n",
        "    cost = compute_cost((y_hat - y_scaled),y_scaled)\n",
        "    dw = (2 / len(y_scaled)) * np.dot(X_scaled.T, (y_hat - y_scaled))\n",
        "    db = (2 / len(y_scaled)) * np.sum(y_hat - y_scaled)\n",
        "\n",
        "    dw, db = compute_gradients_for_multiple_features(X_scaled, y_scaled, y_hat)\n",
        "\n",
        "    w -= learning_rate * dw\n",
        "    b -= learning_rate * db\n",
        "\n",
        "    if epoch % 100 == 0 or epoch == epochs - 1:\n",
        "        print(f\"Epoch {epoch}: Cost = {cost:.4f}, w = {w}, b = {b:.4f}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# --- UNSCALE predictions for real-world output ---\n",
        "y_pred_scaled = np.dot(X_scaled, w) + b\n",
        "y_pred = y_pred_scaled * y_std + y_mean\n",
        "\n",
        "print(\"\\nTesting final model:\")\n",
        "print(\"Predictions:\", np.round(y_pred, 2))\n",
        "print(\"Targets:    \", y)\n",
        "print(\"Difference: \", np.round(y_pred - y, 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKicDcxS3dX0",
        "outputId": "e0dd2dda-e4f8-48c3-b3d4-161dd00030cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "Initial weights: [0. 0. 0.], b = 0\n",
            "--------------------------------------------------\n",
            "Epoch 0: Cost = 4.0000, w = [ 0.01401447  0.01514481 -0.00969374], b = 0.0000\n",
            "Epoch 100: Cost = 1.1062, w = [ 0.48711794  0.4251818  -0.20180018], b = 0.0000\n",
            "Epoch 200: Cost = 1.0133, w = [ 0.54224996  0.43584271 -0.17524399], b = 0.0000\n",
            "Epoch 300: Cost = 1.0030, w = [ 0.55433436  0.43542013 -0.16647035], b = 0.0000\n",
            "Epoch 400: Cost = 1.0009, w = [ 0.55747862  0.43494329 -0.16440218], b = 0.0000\n",
            "Epoch 500: Cost = 1.0003, w = [ 0.55842887  0.4345373  -0.16409032], b = 0.0000\n",
            "Epoch 600: Cost = 1.0002, w = [ 0.55882784  0.43415258 -0.16422223], b = 0.0000\n",
            "Epoch 700: Cost = 1.0002, w = [ 0.55908752  0.43377416 -0.16446534], b = 0.0000\n",
            "Epoch 800: Cost = 1.0002, w = [ 0.55931163  0.43339819 -0.16473581], b = 0.0000\n",
            "Epoch 900: Cost = 1.0002, w = [ 0.55952631  0.43302369 -0.16501253], b = 0.0000\n",
            "Epoch 999: Cost = 1.0002, w = [ 0.55973601  0.43265415 -0.16528742], b = 0.0000\n",
            "--------------------------------------------------\n",
            "Training completed!\n",
            "\n",
            "Testing final model:\n",
            "Predictions: [219.58 280.59 260.1  365.73]\n",
            "Targets:     [220 280 260 366]\n",
            "Difference:  [-0.42  0.59  0.1  -0.27]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m6WZgJZf5Sn2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}